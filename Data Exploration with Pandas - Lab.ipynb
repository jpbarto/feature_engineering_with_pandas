{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML feature engineering explained\n",
    "\n",
    "\"At the end of the day, some machine learning projects succeed and some fail.  What makes the difference?  Easily the most important factor is the features used.\"\n",
    "-- Prof. Pedro Domingos\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook is intended to walk through the process of feature engineering using some common tools and methodologies.  This lab and its associated material is not intended to be exhaustive but an introduction to the concept, challenges, and objectives of feature engineering.  \n",
    "\n",
    "There are numerous writings about feature engineering and readers are encouraged to seek them out beyond this notebook.\n",
    "\n",
    "This notebook assumes you have some proficiency with the Python programming language and will introduce some of the features of the Pandas Python library as well as the scikit-learn library.  A general level of comfort with Jupyter notebooks is also assumed.\n",
    "\n",
    "You will start by downloading a raw data set from the Lending Club.  Once downloaded you will explore the data using various features of the Pandas library and its DataFrame class.  After exploring the data you will begin to transform the data using the same DataFrame class and its manipulation functions.  After the data has been modified to be suitable for the logistic regression classification algorithm, you will use the scikit-learn library to quickly train and test the performance of the data using logistic regression.\n",
    "\n",
    "The goal is to predict the [loan grade](https://www.investopedia.com/terms/l/loan-grading.asp) of a loan based on characeteristics such as loan amount, income, and time of year. \n",
    "\n",
    "This notebook will use a data set made available by the Lending Club.  The data set and data catalog can be found at:\n",
    "\n",
    "https://www.lendingclub.com/info/download-data.action\n",
    "\n",
    "For this notebook use the Q1 2016 CSV data set. \n",
    "\n",
    "To predict \n",
    "\n",
    "### Lab steps\n",
    " 1. Download the data\n",
    " 1. Explore the data\n",
    "     1. Examine the raw data, its shape, and sample values\n",
    "     1. View histograms and correlation matrices of the features\n",
    " 1. Feature engineering\n",
    "     1. Remove redundant or incomplete data\n",
    "     1. Format the data and set its data type\n",
    "     1. Decompose dates, categories, and numerical quantities\n",
    "     1. Normalize and standardize the features\n",
    " 1. Test the features using logistic regression\n",
    " \n",
    " **Extra credit**\n",
    " 1. Use principal component analysis to reduce the number of features\n",
    " 1. Re-evaluate with the new training data set to see the effect of PCA on logistic regression performance\n",
    " \n",
    "\n",
    "## Further reading:\n",
    "\n",
    "https://medium.com/ai%C2%B3-theory-practice-business/top-6-errors-novice-machine-learning-engineers-make-e82273d394db\n",
    "https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "https://machinelearningmastery.com/quick-and-dirty-data-analysis-with-pandas/\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n",
    "https://machinelearningmastery.com/visualize-machine-learning-data-python-pandas/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DOWNLOAD THE SOURCE DATA\n",
    "\n",
    "The data for this tutorial is the Q1 2016 CSV data set available on the [Lending Club website](https://www.lendingclub.com/info/download-data.action).  The commands below will download the data set and unzip it to the local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup",
     "mutate"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://resources.lendingclub.com/LoanStats_2016Q1.csv.zip\n",
    "!unzip LoanStats_2016Q1.csv.zip && rm LoanStats_2016Q1.csv.zip\n",
    "!mkdir data && mv LoanStats_2016Q1.csv data/loan_stats_2016q1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPLORE THE DATA\n",
    "---\n",
    "\n",
    "## Take a peek at the raw data\n",
    "\n",
    "Use a shell command to get the first 5 lines of data from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "read-only",
     "explore"
    ]
   },
   "outputs": [],
   "source": [
    "!head -5 data/loan_stats_2016q1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data into memory using Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is an open source library that provides data structures and tools for data analysis for the Python programming language.\n",
    "\n",
    "In the cell below use the [read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function of pandas to convert the CSV at `data/loan_status_2016q1.csv` to a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "mutate",
     "explore",
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the CSV into a pandas DataFrame\n",
    "DATA_FILE_NAME = 'data/loan_stats_2016q1.csv'\n",
    "# 'df' is a common abbreviation for data frame and is used in many code samples to represent the data object\n",
    "df = pd.read_csv (\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using summary functions to further explore the data\n",
    "\n",
    "Jupyter notebooks will automatically concatenate fields in order to not try to display too much at once.  Using the `set_option` function of Pandas we will configure the notebook to display up to 145 columns of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure",
     "mutate"
    ]
   },
   "outputs": [],
   "source": [
    "# the data is 145 columns wide, we will need to allow for all the columns when displaying the data\n",
    "pd.set_option ('display.max_columns', 145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below use the [info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html) function to display information such as record count, number of columns, data types, and how much memory is used by the DataFrame that now houses the CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "read-only"
    ]
   },
   "outputs": [],
   "source": [
    "# display a summary of the data frame shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the DataFrame contains 133,889 rows of data with 145 columns.  It has two data types, float and object, and is using 148 MB of RAM.\n",
    "\n",
    "Next obtain 5 example rows of data using the [sample](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "read-only",
     "explore"
    ]
   },
   "outputs": [],
   "source": [
    "# print a tabulated list of 5 random rows of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the far left is the row number for every sample record.  You will also see some fields have no value or are shown as 'NaN'.  This indicates missing data which may or may not be acceptable given your use case.\n",
    "\n",
    "Next use the [describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) function to list common calculations such as average, standard deviation, min, and max values for all numerical fields in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "read-only",
     "explore"
    ]
   },
   "outputs": [],
   "source": [
    "# list common statistics per column for the data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe function will provide statistics for every column of a float data type.  This will include how many values are in the column (vs NaN values), the average, standard deviation, and different percentiles.\n",
    "\n",
    "Next list the data types for each column using the `dtypes` attribute of the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure",
     "explore",
     "read-only"
    ]
   },
   "outputs": [],
   "source": [
    "# retrieve a list of the data types for each column\n",
    "pd.set_option ('display.max_rows', 145)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see all the various fields and their data type lets get a sample of the fields that have the ``object`` data type.  Perhaps we can convert these to a data type that is better suited for machine learning.  Use the [select_dtypes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html) function to show a sample of 5 values for each field which is of type `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "explore",
     "read-only"
    ]
   },
   "outputs": [],
   "source": [
    "# show those columns that have an 'object' data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the values from the sample above many of these are best suited as a ``category`` data type, a field with a finite set of potential values.  Others are percentages that can be converted to floating point numbers or dates.\n",
    "\n",
    "### Data value distribution\n",
    "\n",
    "Histograms can be used to get a feel for the distribution of values within any field in a data set.  Using the ``hist`` function of a Pandas DataFrame you can easily see how the values are distributed.  Look for those fields that have a Gaussian, exponential, or skewed distribution.  This will help you identify outliers in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.hist (figsize=(25,25))\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "\n",
    "Using a correlation matrix you can quickly get a feel for where correlation may reside in your data set.  Those fields that are yellow demonstrate a high correlation and those that are blue demonstrate a high inverse correlation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df_float = df.select_dtypes(include='float64')\n",
    "col_names = df_float.columns.tolist()\n",
    "\n",
    "correlations = df_float.corr()\n",
    "\n",
    "# plot correlation matrix\n",
    "fig = plt.figure(figsize=(25,25))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(col_names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(col_names)\n",
    "ax.set_yticklabels(col_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields that are white indicate null values in the data set.\n",
    "\n",
    "### Data exploration conclusion\n",
    "\n",
    "You should now have a sense of the data that is in the data set, how many columns there are, how many rows, and have a sense of what data types are in the data set.  In addition you should have an appreciation for the quality of the data as it stands.  Many fields are empty or have ``object`` or string value types that we will need to convert for them to be useful for ML purposes.  Lets now dig into the data to determine which fields are useful, which can be removed, and which need to be modified to help describe our problem space and enable us to develop an effective ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing\n",
    "\n",
    "### Remove unnecessary or redundant columns\n",
    "\n",
    "Looking at the ``sample`` displays above it appears that some of the columns may have duplicate data or data that is perhaps irrelevant to the problem we are trying to solve.  Lets explore these further and remove them if necessary.\n",
    "\n",
    "Use the ``unique`` function for any column of the data to get a list of the potential values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.purpose.unique ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same [unique](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) function list the values for the `title` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `title` and `purpose` columns appear to be redundant.  Use the [drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) function to delete the `title` column and the [upper](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.upper.html) function to capitalize the values in the `purpose` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be trying to derive value from the job titles so for now, lets remove them from our feature set using the `drop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns that contain 10% or more NaN values\n",
    "\n",
    "We want to ensure that of the data we have that it is of a certain quality.  For any columns that contain more than 10% of null or empty values remove the column as it doesn't contain enough data to be of value.\n",
    "\n",
    "Lets start by using the [isna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isna.html) and [sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html) functions to list each column and count the NaN values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DataFrame before null removal\")\n",
    "df.info (verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the [dropna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) function to remove those columns that have more than 10% of NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DataFrame AFTER null removal\")\n",
    "df.info ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the before and after we can see that we have gone from 143 columns to 97 columns, more than 45 columns had more than 10% empty data!  \n",
    "\n",
    "**Note**: 10% is a threshold set for the purposes of this lab, the threshold that is suitable for your data will be dependent upon the algorithm you are training with and the data you are using.\n",
    "\n",
    "### Remove rows that contain ALL NaN values\n",
    "\n",
    "We have now cleared columns with too much empty data.  Lets now use the same `dropna` function to remove any rows that contain nothing but NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting\n",
    "\n",
    "Use the `sample` function again to retrieve another 5 example rows of data.  Look at the values and determine where unnecessary formatting can be removed such as '%' symbols or labels such as 'years'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary formatting from field values in prep for setting data types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the term column\n",
    "\n",
    "Using the `unique` function to assess the `term` column - is there a unit of measure that can be stripped to convert the value into a numerical data type?  Using the [apply](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) function strip the label from the text and convert it to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that term contains 133887 values, but there are 133889 rows of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the string value of term to an int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the int_rate column\n",
    "\n",
    "The interest rate is also being stored as an object at the moment for the sake of its '%' sign, remove this and format the number as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the interest rate field to a float value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the revol_util column\n",
    "\n",
    "Like the interest rate the ``revol_util`` column also contains a '%' sign, remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the revol_util column, it contains '%' signs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup dates\n",
    "\n",
    "The month is a cyclical value, after December comes January, so December (12) is closer to January (1) than perhaps October (10).  1 month between Dec and Jan vs 3 months between October and January.  We need to represent this cyclical nature using the geometrical functions sin and cos.\n",
    "\n",
    "For more read http://blog.davidkaleko.com/feature-engineering-cyclical-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a month / year date to an individual month and year column\n",
    "from numpy import cos, sin, pi\n",
    "\n",
    "def decompose_date (pd_df, column_name):\n",
    "    date_column = pd.to_datetime(pd_df[column_name])\n",
    "    month = date_column.dt.month\n",
    "    pd_df[column_name +\"_month_cos\"] = cos ((month-1)*(2.0*pi/12)) # subtract 1 so that jan is month 0\n",
    "    pd_df[column_name +\"_month_sin\"] = sin ((month-1)*(2.0*pi/12))\n",
    "    pd_df[column_name +\"_year\"] = date_column.dt.year.astype (float)\n",
    "    pd_df[column_name +\"_absolute\"] = pd.to_numeric(date_column.dt.strftime ('%s'), errors = 'coerce') # store the unix timestamp\n",
    "    pd_df.drop(columns=[column_name], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_date (df, 'issue_d')\n",
    "decompose_date (df, 'earliest_cr_line')\n",
    "decompose_date (df, 'last_pymnt_d')\n",
    "decompose_date (df, 'last_credit_pull_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data types\n",
    "\n",
    "We have refined the values in our data but based on the data types listing above you can see that there are still a few ``object`` data types.  We can go through each one of these types by hand and determine if we can further refine them to float or int values but for now lets set them all to categorical items.  A category is a value that has a finite set of values such as payment types ('cash', 'credit', 'check') or day of the week ('monday', 'tuesday', etc).  Convert all remaining ``object`` types to a ``category`` type.\n",
    "\n",
    "When fed to an ML algorithm the ``category`` data types will be 'one hot' encoded to a numerical value.  In the example of payment types this will be a 3 digit vector where a value of 1 is set in place of the correct value and others are set to 0.  For example a payment type of 'credit' would be one hot encoded to an array such as [0, 1, 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set categorical columns\n",
    "\n",
    "Use the [select_dtypes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html) function to obtain an iterator over all features of type `object`.  Then use the [astype](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) function to convert each to a type of `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any column of type object that does not contain NaN, set its data type to categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanse conclusion\n",
    "\n",
    "You have now removed a majority of the NaN or empty values, formatted the data to be of a data type that an ML algorithm will understand and you converted values to meaningful float values where appropriate.  The next step is to use scaling and normalization to bring values within relative magnitude of one another.\n",
    "\n",
    "\n",
    "## Scale the features\n",
    "\n",
    "Looking at a sample of the data above you can see that the values in the data can vary widely in terms of their magnitude, variability, and units.  For instance ``loan_amount`` and ``installment`` are on the order of thousands and hundreds respectively, ``term`` meanwhile has a unit of months and ``delinq_2yrs`` is a count.  For some machine learning algorithms these differences can cause training to take longer or not converge at all.  By scaling these features you can give them a similar magnitude without affecting the value's distribution.\n",
    "\n",
    "For a good introduction to feature scaling, how it works, what it does, and when to use it see:\n",
    "\n",
    "https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "The functions `normalize` and `standardize` are defined here for simplicity and clarity.  Many Python frameworks such as scikit-learn provide functions out of the box for scaling and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize (series):\n",
    "    return (series - series.min ())/(series.max () - series.min ())\n",
    "\n",
    "def standardize (series):\n",
    "    return (series - series.mean())/(series.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the distribution of the feature is not affected by scaling use the ``plot`` function of the Pandas DataFrame to display the distribution before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df.loan_amnt.plot.kde (title = 'loan_amnt distribution pre-scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `select_dtypes` function again to obtain a reference to all features of type `float64` or `int64`.  Use the `standardize` function above to reduce the magnitude of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "    \n",
    "df.loan_amnt.plot.kde (title = 'loan_amnt distribution post-scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values of the X and Y axis have changed but the distribution of the data over the range has remained static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the field has not changed and that the variations remain the same.  However the Standard Deviation has been equalized across the fields bringing the values into a similar magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "# TEST THE DATA\n",
    "\n",
    "Now that the data has been cleaned and the features engineered, create a training and testing data set to quickly assess how well the engineered data performs using logistic regression.\n",
    "\n",
    "We will use 80% of the overall data set for training and the remaining 20% of the data for testing purposes.  As logistic regression is a supervised ML algorithm we will provide it with the 'grade' feature as our label.\n",
    "\n",
    "You will need to perform the 'one hot' encoding of the categorical data columns.  To do this use the [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function to convert all categorical columns in the data set.\n",
    "\n",
    "Rather than have the logistic regression algorithm predict a `one hot` encoded vector lets use the [argmax](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.argmax.html) function to reduce the `one hot` encoding to a number such as 1, 2, 3, 4, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_index = int (df.shape[0] * 0.8)\n",
    "\n",
    "# Note, SKLearn does not do well with NaN values, remove any and all\n",
    "df.dropna(axis=1, inplace=True)\n",
    "\n",
    "# split the features from the labels\n",
    "labels = df.pop('grade')\n",
    "\n",
    "df = pd.get_dummies (df)\n",
    "labels = pd.get_dummies (labels)\n",
    "labels = labels.values.argmax (axis = 1)\n",
    "\n",
    "train_df = df[:train_test_split_index]\n",
    "train_labels = labels[:train_test_split_index]\n",
    "print (\"Training data has {} rows\".format (train_df.shape[0]))\n",
    "\n",
    "test_df = df[train_test_split_index:]\n",
    "test_labels = labels[train_test_split_index:]\n",
    "print (\"Test data has {} rows\".format (test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using logistic regression\n",
    "\n",
    "scikit-learn has many common classifiers pre-built in the library.  Lets now use the logistic regression algorithm to train a model based upon our feature data and assess how well the trained model performs on the test data set.\n",
    "\n",
    "The logistic regression algorithm is instantiated for you, use the [fit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) function to train the algorithm based upon your training data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg = LogisticRegression (solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lreg.fit ( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your model trained lets now use the test data set to evaluate the performance of the model on yet unseen observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print (\"Accuracy for logistic regression on data set with {} features\".format (train_df.shape[1]))\n",
    "print (lreg.score (test_df, test_labels))\n",
    "\n",
    "predictions = lreg.predict (test_df)\n",
    "print (\"Precision, recall, F1 score\")\n",
    "print (precision_recall_fscore_support (test_labels, predictions, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA CREDIT: Principal Component Analysis (PCA)\n",
    "\n",
    "When encoded our data set contains over 1000 features.  Many other data sets may contain 10s or 100s of thousands of features, some even go into the millions of features for every observation.  Unless you have a deep understanding of every field in your data set and how it relates to your target value tools like PCA can aid greatly in determineing those features that are the most influential and important for your machine learning model.\n",
    "\n",
    "PCA is an algorithm that observes the relationship between individual features and the target label.  Using Principal Component Analysis you can reduce your feature set to be half its size, ideally without losing any model accuracy. Such a reduction in size will allow your models to train more quickly.\n",
    "\n",
    "In addition to reducing the dimensionality of your data PCA can also ensure that all features are independent of one another.  Some machine learning algorithms will require that features be independent of one another.\n",
    "\n",
    "Lets use the PCA class from SciKit-Learn to reduce our feature set while still being able to describe with a high degree of accuracy the target labels.\n",
    "\n",
    "**Note:** Principal component analysis will require all numerical values and will not accept NaN.  Further engineer the data set to remove null values and convert the category data to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA (0.99) # PCA should choose the components that account for 99% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The training data set holds {} records with {} features\".format (train_df.shape[0], train_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [fit](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit) function of the PCA algorithm, reduce the number of features to the optimal number needed to account for 99% of the variance in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca.fit ( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use the [transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform) method to modify the training and test data sets to the reduced feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pca.transform ( ... )\n",
    "test_df = pca.transform( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"After PCA analysis the data set contains {} rows with {} features\".format (train_df.shape[0], train_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after analysis PCA has reduced the dimensionality of the data set to less than 1/10th the size of the original feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lreg = LogisticRegression (solver = 'lbfgs')\n",
    "lreg.fit (X=train_df, y=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a logistic regression model on the reduced data set required less time than the full feature set.  The model trained more quickly, lets see what has happened to the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy for logistic regression on reduced data set with {} features\".format (train_df.shape[1]))\n",
    "print (lreg.score (test_df, test_labels))\n",
    "\n",
    "predictions = lreg.predict (test_df)\n",
    "print (\"Precision, recall, F1 score\")\n",
    "print (precision_recall_fscore_support (test_labels, predictions, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little to no impact to model accuracy and precision while cutting training time down by 66%.\n",
    "\n",
    "## Output for training\n",
    "\n",
    "Now that the features have been selected, cleansed, and scaled lets output them for consumption by a training algorithm.  This can be easily done using Pandas functions like [to_parquet](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_parquet.html) or [to_csv](http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.to_csv.html).  To see what the results would look like use the ``get_dummies`` function to perform the one hot encoding of the categories to vectors.  This will give you an example of the data which is ultimately used to train your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame (train_df).to_csv ('data/pca_train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame (train_labels).to_csv ('data/train_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/pca_train_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/train_labels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of lab\n",
    "\n",
    "You have now downloaded a raw data set and used tools such as Pandas to explore the data, engineer a feature set from the raw data, and optimize the data set for use with logistic regression.  The next step from here is to take the work you've developed in this Jupyter notebook and make it a part of an extraction, transformation, and loading process for reuse.  Equally the logic applied to the raw data fields will also need to be applied to future requests to the trained model.  New observartions will need to be transformed in the same manner as the training data so that the trained model can recognize it.  This can be done at the client side or within the model itself and is up to the development team.  \n",
    "\n",
    "While logistic regression appears to be 99.9% accurate in calculating loan grade based upon the data provided what other features can you see being derived from this data set?  What are your thoughts on what data fields are the most relevant and how could they further be transformed to more accurately reflect reality?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
